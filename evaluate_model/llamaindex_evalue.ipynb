{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llamaindex 评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    get_response_synthesizer,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置嵌入模型和语言模型\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"yxl/m3e:latest\")\n",
    "# Settings.llm = Ollama(model=\"qwen2.5:0.5b\", request_timeout=360)\n",
    "llm = Ollama(model=\"qwen2.5:0.5b\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Chroma 客户端，指定数据存储路径为当前目录下的 chroma_db 文件夹\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 获取或创建名为 \"quickstart\" 的集合，如果该集合不存在，则创建它\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# 使用上述集合创建一个 ChromaVectorStore 实例，以便 llama_index 可以与 Chroma 集合进行交互\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# 创建一个存储上下文，指定向量存储为刚刚创建的 ChromaVectorStore 实例\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文档\n",
    "documents = SimpleDirectoryReader(\"C:/Users/Admin/Desktop/Data/\").load_data()\n",
    "\n",
    "# 构建索引\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    transformations=[SentenceSplitter(chunk_size=256)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置检索器\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,  # 返回最相似的前 n 个文档片段\n",
    ")\n",
    "\n",
    "# 配置响应合成器\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# 组装查询引擎\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估 RAG 应用需要用到几个评估实体，分别是：\n",
    "\n",
    "- Question: 指用户输入的问题，RAG 应用通过问题检索到相关的文档上下文\n",
    "- Context: 指检索到的文档上下文，RAG 应用检索到相关文档后会将这些上下\n",
    "  文结合用户问题一起提交给 LLM，最后生成答案\n",
    "- Answer: 指生成的答案，RAG 应用将问题和上下文提交给 LLM 后，LLM 会\n",
    "  根据这些信息来生成答案\n",
    "- Grouth Truth: 指人工标注的正确答案，利用这个实体可以对生成的答案进\n",
    "  行分析，从而得到评估结果，在 LlamaIndex 中，这个实体叫做 Reference Answer\n",
    "\n",
    "其中 Question 和 Ground Truth 通过用户提供，Context 通过检索得到，Answer\n",
    "是由 LLM 生成，后面我们在讲解的时候会沿用这些实体名称。在 LlamaIndex 中提供\n",
    "了生成测试数据集的功能，可以帮助我们快速生成测试数据集，无需人工干预。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "from llama_index.core.evaluation import ContextRelevancyEvaluator\n",
    "from llama_index.core.evaluation import AnswerRelevancyEvaluator\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n",
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "answer_relevancy_evaluator = AnswerRelevancyEvaluator(llm)\n",
    "context_relevancy_evaluator = ContextRelevancyEvaluator(llm)\n",
    "relevant_evaluator = RelevancyEvaluator(llm)\n",
    "correctness_evaluator = CorrectnessEvaluator(llm)\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm)\n",
    "pairwiseComparisonEvaluator = PairwiseComparisonEvaluator(llm)\n",
    "\n",
    "runner = BatchEvalRunner(\n",
    "    evaluators={\n",
    "        \"answer_relevancy\": answer_relevancy_evaluator,\n",
    "        \"context_relevancy\": context_relevancy_evaluator,\n",
    "        \"relevancy\": relevant_evaluator,\n",
    "        \"correctness\": correctness_evaluator,\n",
    "        \"faithfulness\": faithfulness_evaluator,\n",
    "        \"pairwiseComparisonEvaluator\": pairwiseComparisonEvaluator,\n",
    "    },\n",
    "    workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "Settings.llm = llm\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "engine = vector_index.as_query_engine()\n",
    "response = engine.query(question)\n",
    "answer = str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [example.query for example in examples]\n",
    "ground_truths = [example.reference_answer for example in examples]\n",
    "metrics_results = runner.evaluate_queries(\n",
    "    engine, queries=questions, reference=ground_truths\n",
    ")\n",
    "\n",
    "for metrics in metrics_results.keys():\n",
    "    print(f\"metrics: {metrics}\")\n",
    "    eval_results = metrics_results[metrics]\n",
    "    for eval_result in eval_results:\n",
    "        print(f\"score: {eval_result.score}\")\n",
    "        print(f\"feedback: {eval_result.feedback}\")\n",
    "        if eval_result.passing is not None:\n",
    "            print(f\"passing: {eval_result.passing}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
