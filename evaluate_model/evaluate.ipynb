{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准确性（Accuracy）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_answers, predicted_answers):\n",
    "    \"\"\"\n",
    "    计算准确性（Accuracy）\n",
    "\n",
    "    :param true_answers: 真实的答案（列表形式）\n",
    "    :param predicted_answers: 模型预测的答案（列表形式）\n",
    "    :return: 准确性值\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(true_answers)\n",
    "\n",
    "    for true, predicted in zip(true_answers, predicted_answers):\n",
    "        if true == predicted:  # 如果生成的答案与真实答案完全匹配\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 精确度（Precision）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def precision(true_answers, predicted_answers):\n",
    "    \"\"\"\n",
    "    计算精确度（Precision）\n",
    "\n",
    "    :param true_answers: 真实的答案（列表形式）\n",
    "    :param predicted_answers: 模型预测的答案（列表形式）\n",
    "    :return: 精确度值\n",
    "    \"\"\"\n",
    "    # 假设答案已经分词或标记化，转为词汇级别的比较\n",
    "    true_answers = [word for ans in true_answers for word in ans.split()]\n",
    "    predicted_answers = [word for ans in predicted_answers for word in ans.split()]\n",
    "\n",
    "    return precision_score(\n",
    "        true_answers, predicted_answers, average=\"binary\"\n",
    "    )  # 根据具体任务调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 召回率（Recall）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "def recall(true_answers, predicted_answers):\n",
    "    \"\"\"\n",
    "    计算召回率（Recall）\n",
    "\n",
    "    :param true_answers: 真实的答案（列表形式）\n",
    "    :param predicted_answers: 模型预测的答案（列表形式）\n",
    "    :return: 召回率值\n",
    "    \"\"\"\n",
    "    # 假设答案已经分词或标记化，转为词汇级别的比较\n",
    "    true_answers = [word for ans in true_answers for word in ans.split()]\n",
    "    predicted_answers = [word for ans in predicted_answers for word in ans.split()]\n",
    "\n",
    "    return recall_score(\n",
    "        true_answers, predicted_answers, average=\"binary\"\n",
    "    )  # 根据具体任务调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 值（F1-Score）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def f1(true_answers, predicted_answers):\n",
    "    \"\"\"\n",
    "    计算 F1 值（F1-Score）\n",
    "\n",
    "    :param true_answers: 真实的答案（列表形式）\n",
    "    :param predicted_answers: 模型预测的答案（列表形式）\n",
    "    :return: F1 值\n",
    "    \"\"\"\n",
    "    # 假设答案已经分词或标记化，转为词汇级别的比较\n",
    "    true_answers = [word for ans in true_answers for word in ans.split()]\n",
    "    predicted_answers = [word for ans in predicted_answers for word in ans.split()]\n",
    "\n",
    "    return f1_score(\n",
    "        true_answers, predicted_answers, average=\"binary\"\n",
    "    )  # 根据具体任务调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "def bleu_score(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    计算BLEU分数\n",
    "\n",
    "    :param reference: 真实答案（列表，分词后的单词）\n",
    "    :param hypothesis: 生成的答案（列表，分词后的单词）\n",
    "    :return: BLEU得分\n",
    "    \"\"\"\n",
    "    return sentence_bleu([reference], hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import mkdtemp\n",
    "from pyrouge import Rouge155\n",
    "\n",
    "\n",
    "def rouge_score(reference_text, prediction_text):\n",
    "    \"\"\"\n",
    "    计算ROUGE分数\n",
    "\n",
    "    Args:\n",
    "        reference_text (str): 参考文本(原文)\n",
    "        prediction_text (str): 预测文本(生成的摘要)\n",
    "\n",
    "    Returns:\n",
    "        dict: ROUGE分数,包含precision、recall、f-score等指标\n",
    "    \"\"\"\n",
    "    # 创建临时目录\n",
    "    temp_dir = mkdtemp()\n",
    "    system_dir = os.path.join(temp_dir, \"system\")\n",
    "    model_dir = os.path.join(temp_dir, \"model\")\n",
    "    os.makedirs(system_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "    # 写入文本文件\n",
    "    with open(os.path.join(system_dir, \"prediction.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prediction_text)\n",
    "    with open(os.path.join(model_dir, \"reference.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(reference_text)\n",
    "\n",
    "    # 初始化Rouge155\n",
    "    rouge = Rouge155()\n",
    "    rouge.system_dir = system_dir\n",
    "    rouge.model_dir = model_dir\n",
    "    rouge.system_filename_pattern = \"prediction.txt\"\n",
    "    rouge.model_filename_pattern = \"reference.txt\"\n",
    "\n",
    "    # 运行评估\n",
    "    scores = rouge.evaluate()\n",
    "    scores_dict = rouge.output_to_dict(scores)\n",
    "\n",
    "    # 清理临时文件\n",
    "    os.remove(os.path.join(system_dir, \"prediction.txt\"))\n",
    "    os.remove(os.path.join(model_dir, \"reference.txt\"))\n",
    "    os.rmdir(system_dir)\n",
    "    os.rmdir(model_dir)\n",
    "    os.rmdir(temp_dir)\n",
    "\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METEOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import meteor_score\n",
    "\n",
    "\n",
    "def meteor(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    计算METEOR分数\n",
    "\n",
    "    :param reference: 真实答案（字符串）\n",
    "    :param hypothesis: 生成的答案（字符串）\n",
    "    :return: METEOR得分\n",
    "    \"\"\"\n",
    "    return meteor_score.meteor_score([reference], hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "true_answers = [\n",
    "    \"这是第一个真实答案。\",\n",
    "    \"这是第二个真实答案。\",\n",
    "    \"这是第三个真实答案。\"\n",
    "]\n",
    "\n",
    "predicted_answers = [\n",
    "    \"这是第一个预测答案。\",\n",
    "    \"这是第二个预测答案。\",\n",
    "    \"这是第三个预测答案。\"\n",
    "]\n",
    "\n",
    "# 分词处理\n",
    "true_answers = [\" \".join(jieba.cut(ans)) for ans in true_answers]\n",
    "predicted_answers = [\" \".join(jieba.cut(ans)) for ans in predicted_answers]\n",
    "\n",
    "print(true_answers)\n",
    "print(predicted_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloom_ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
